{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=too-many-arguments, too-many-locals\n",
    "\"\"\" Variational inference \"\"\"\n",
    "import math\n",
    "import functools\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "from scipy.special import gammaln\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "VariationalParameter = namedtuple('VariationalParameter',\n",
    "                                  ['mean', 'rho', 'eps'])\n",
    "\n",
    "\n",
    "def evaluate(variational_parameter):\n",
    "    \"\"\" Evaluates the current value of a variational parameter.\n",
    "    Returns mean + log(1 + e^rho) * eps\n",
    "    :args variational_parameter: the variational parameter\n",
    "    :returns: the value of the variational parameter\n",
    "    \"\"\"\n",
    "    assert isinstance(variational_parameter, VariationalParameter), \\\n",
    "        \"Incorrect type.\"\n",
    "    return variational_parameter.mean + \\\n",
    "        (1 + variational_parameter.rho.exp()).log() * variational_parameter.eps\n",
    "\n",
    "\n",
    "def rebuild_parameters(dico, module, epsilon_setting):\n",
    "    \"\"\" Rebuild parameters.\n",
    "    Build the computational graph corresponding to\n",
    "    the computations of the parameters of the given module,\n",
    "    using the corresponding variational parameters in dico,\n",
    "    and the rule used to sample epsilons. If the module has\n",
    "    submodules, corresponding subcomputational graphs are also\n",
    "    built.\n",
    "    Typically, if a module has a parameter weight, weight should\n",
    "    appear in dico, and the parameter will be rebuilt as\n",
    "    module.weight = dico['weight'].mean + (1+dico['weight'].rho.exp()).log() *\n",
    "            dico['weight'].eps\n",
    "    :args dico: a 'tree' dictionnary that contains variational\n",
    "    parameters for the current module, and subtrees for submodules\n",
    "    :args module: the module whose parameters are to be rebuilt\n",
    "    :args epsilon_settings: how epsilons ought to be drawn\n",
    "    \"\"\"\n",
    "\n",
    "    for name, p in dico.items():\n",
    "        if isinstance(p, VariationalParameter):\n",
    "            if p.eps is None:\n",
    "                dico[name] = p._replace(eps=Variable(p.mean.data.clone()))\n",
    "            epsilon_setting(name, dico[name])\n",
    "            setattr(module, name, evaluate(dico[name]))\n",
    "        elif p is None:\n",
    "            setattr(module, name, None)\n",
    "        else:\n",
    "            rebuild_parameters(p, getattr(module, name), epsilon_setting)\n",
    "\n",
    "\n",
    "def prior_std(p):\n",
    "    \"\"\" Compute a reasonable prior standard deviation for parameter p.\n",
    "    :args p: the parameter\n",
    "    :return: the resulting std\n",
    "    \"\"\"\n",
    "    stdv = 1\n",
    "    if p.dim() > 1:\n",
    "        for i in range(p.dim() - 1):\n",
    "            stdv = stdv * p.size()[i + 1]\n",
    "        stdv = 1 / math.sqrt(stdv)\n",
    "    else:\n",
    "        stdv = 1e-2\n",
    "    return stdv\n",
    "\n",
    "\n",
    "def sub_prior_loss(dico):\n",
    "    \"\"\" Compute the KL divergence between prior and parameters for\n",
    "    all Variational Parameters in the tree dictionary dico.\n",
    "    :args dico: tree dictionary\n",
    "    :return: KL divergence between prior and current\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for p in dico.values():\n",
    "        if isinstance(p, VariationalParameter):\n",
    "            mean = p.mean\n",
    "            std = (1 + p.rho.exp()).log()\n",
    "            std_prior = prior_std(mean)\n",
    "            loss += (-(std / std_prior).log() +\n",
    "                     (std.pow(2) + mean.pow(2)) /\n",
    "                     (2 * std_prior ** 2) - 1 / 2).sum()\n",
    "        else:\n",
    "            loss += sub_prior_loss(p)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sub_entropy(dico):\n",
    "    \"\"\" Compute the entropy of the parameters for all Variational\n",
    "    Parameters in the tree dictionary dico.\n",
    "    :args dico: tree dictionary\n",
    "    :returns: Entropy of the current distribution\n",
    "    \"\"\"\n",
    "    entropy = 0.\n",
    "    for _, p in dico.items():\n",
    "        if isinstance(p, VariationalParameter):\n",
    "            std = (1 + p.rho.exp()).log()\n",
    "            n = np.prod(std.size())\n",
    "            entropy += std.log().sum() + .5 * n * (1 + np.log(2 * np.pi))\n",
    "        else:\n",
    "            entropy += sub_entropy(p)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def sub_conjprior(dico, alpha_0, beta_0, mu_0, kappa_0):\n",
    "    \"\"\" Compute an estimation of the KL divergence between the conjugate\n",
    "    prior and parameters for all Variational Parameters in the tree\n",
    "    dictionary dico.\n",
    "    :args dico: tree dictionary\n",
    "    :args alpha_0: hyperparameter of the conjugate prior\n",
    "    :args beta_0: hyperparameter of the conjugate prior\n",
    "    :args mu_0: hyperparameter of the conjugate prior\n",
    "    :args kappa_0: hyperparameter of the conjugate prior\n",
    "    :return: estimation of the KL divergence between prior and current\n",
    "    \"\"\"\n",
    "    logprior = 0.\n",
    "    for _, p in dico.items():\n",
    "        if isinstance(p, VariationalParameter):\n",
    "            theta = evaluate(p)\n",
    "            S = (theta.mean() - mu_0).norm() ** 2\n",
    "            V = (theta - theta.mean()).norm() ** 2\n",
    "            n = np.prod(theta.size())\n",
    "            alpha_n = alpha_0 + n / 2\n",
    "            kappa_n = kappa_0 + n\n",
    "            beta_n = beta_0 + V / 2 + S * (kappa_0 * n) / (2 * kappa_n)\n",
    "            logprior += - beta_n.log() * alpha_n + alpha_0 * np.log(beta_0) + \\\n",
    "                gammaln(alpha_n) - gammaln(alpha_0) + \\\n",
    "                .5 * np.log(kappa_0 / kappa_n) - .5 * n * np.log(2 * np.pi)\n",
    "\n",
    "        else:\n",
    "            logprior += sub_conjprior(\n",
    "                p, alpha_0, beta_0, mu_0, kappa_0)\n",
    "    return logprior\n",
    "\n",
    "\n",
    "def sub_conjpriorknownmean(dico, mean, alpha_0, beta_0):\n",
    "    \"\"\" Compute an estimation of the KL divergence between the conjugate\n",
    "    prior when the mean is known and parameters for all Variational\n",
    "    Parameters in the tree dictionary dico.\n",
    "    :args dico: tree dictionary\n",
    "    :args mean: known mean for the conjugate prior\n",
    "    :args alpha_0: hyperparameter of the conjugate prior\n",
    "    :args beta_0: hyperparameter of the conjugate prior\n",
    "    :return: estimation of the KL divergence between prior and current\n",
    "    \"\"\"\n",
    "    logprior = 0.\n",
    "    for _, p in dico.items():\n",
    "        if isinstance(p, VariationalParameter):\n",
    "            theta = evaluate(p)\n",
    "            S = (theta - mean).norm() ** 2\n",
    "            n = np.prod(theta.size())\n",
    "            alpha_n = alpha_0 + n / 2\n",
    "            beta_n = beta_0 + S / 2\n",
    "            logprior += - beta_n.log() * alpha_n + \\\n",
    "                gammaln(alpha_n) - gammaln(alpha_0) + \\\n",
    "                alpha_0 * np.log(beta_0) - .5 * n * np.log(2 * np.pi)\n",
    "        else:\n",
    "            logprior += sub_conjpriorknownmean(\n",
    "                p, mean, alpha_0, beta_0)\n",
    "    return logprior\n",
    "\n",
    "\n",
    "def sub_mixtgaussprior(dico, sigma_1, sigma_2, pi):\n",
    "    \"\"\" Compute an estimation of the KL divergence between the prior\n",
    "    defined by the mixture of two gaussian distributions\n",
    "    for all Variational Parameters in the tree dictionary dico.\n",
    "    More details on this prior and the notations can be found in :\n",
    "    \"Weight Uncertainty in Neural Networks\" Blundell et al, 2015\n",
    "    https://arxiv.org/pdf/1505.05424.pdf\n",
    "    :args dico: tree dictionary\n",
    "    :args sigma_1: std of the first gaussian in the mixture\n",
    "    :args sigma_2: std of the second gaussian in the mixture\n",
    "    :args pi: probability of the first gaussian in the mixture\n",
    "    :return: estimation of the KL divergence between prior and current\n",
    "    \"\"\"\n",
    "    logprior = 0.\n",
    "    for _, p in dico.items():\n",
    "        if isinstance(p, VariationalParameter):\n",
    "            theta = evaluate(p)\n",
    "            n = np.prod(theta.size())\n",
    "            theta2 = theta ** 2\n",
    "            pgauss1 = (- theta2 / (2. * sigma_1 ** 2)).exp() / sigma_1\n",
    "            pgauss2 = (- theta2 / (2. * sigma_2 ** 2)).exp() / sigma_2\n",
    "            logprior += (pi * pgauss1 + (1 - pi) * pgauss2 + 1e-8).log().sum()\n",
    "            logprior -= n / 2 * np.log(2 * np.pi)\n",
    "        else:\n",
    "            logprior += sub_mixtgaussprior(\n",
    "                p, sigma_1, sigma_2, pi)\n",
    "    return logprior\n",
    "\n",
    "\n",
    "class Variationalize(nn.Module):\n",
    "    \"\"\" Build a Variational model over the model given as input.\n",
    "    Variationalize changes all parameters of the given model\n",
    "    to allow learning of a gaussian distribution over the\n",
    "    parameters using Variational inference. For more information,\n",
    "    see e.g. https://papers.nips.cc/paper/4329-practical-variational\n",
    "    -inference-for-neural-networks.pdf.\n",
    "    :args model: the model on which VI is to be performed\n",
    "    :args zero_mean: if True, sets initial mean to 0, else\n",
    "        keep model initial mean\n",
    "    :args learn_mean: if True, learn the posterior mean\n",
    "    :args learn_rho: if True, learn the posterior rho\n",
    "    \"\"\"\n",
    "    def __init__(self, model, zero_mean=True, learn_mean=True, learn_rho=True):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        self.dico = OrderedDict()\n",
    "        self._variationalize_module(self.dico, self.model, '', zero_mean,\n",
    "                                    learn_mean, learn_rho)\n",
    "        self._prior_loss_function = functools.partial(\n",
    "            sub_prior_loss,\n",
    "            dico=self.dico)\n",
    "\n",
    "    def _variationalize_module(self, dico, module, prefix, zero_mean,\n",
    "                               learn_mean, learn_rho):\n",
    "        to_erase = []\n",
    "        paras = module._parameters.items()  # pylint: disable=protected-access\n",
    "        for name, p in paras:\n",
    "            if p is None:\n",
    "                dico[name] = None\n",
    "            else:\n",
    "                stdv = prior_std(p)\n",
    "                init_rho = math.log(math.exp(stdv) - 1)\n",
    "\n",
    "                init_mean = p.data.clone()\n",
    "                if zero_mean:\n",
    "                    init_mean.fill_(0)\n",
    "\n",
    "                dico[name] = VariationalParameter(\n",
    "                    Parameter(init_mean),\n",
    "                    Parameter(p.data.clone().fill_(init_rho)),\n",
    "                    None)\n",
    "\n",
    "                if learn_mean:\n",
    "                    self.register_parameter(prefix + '_' + name + '_mean',\n",
    "                                            dico[name].mean)\n",
    "                if learn_rho:\n",
    "                    self.register_parameter(prefix + '_' + name + '_rho',\n",
    "                                            dico[name].rho)\n",
    "\n",
    "            to_erase.append(name)\n",
    "\n",
    "        for name in to_erase:\n",
    "            delattr(module, name)\n",
    "\n",
    "        for mname, sub_module in module.named_children():\n",
    "            sub_dico = OrderedDict()\n",
    "            self._variationalize_module(sub_dico, sub_module,\n",
    "                                        prefix + ('_' if prefix else '') +\n",
    "                                        mname, zero_mean,\n",
    "                                        learn_mean, learn_rho)\n",
    "            dico[mname] = sub_dico\n",
    "\n",
    "    def set_prior(self, prior_type, **prior_parameters):\n",
    "        \"\"\" Change the prior to be used.\n",
    "        Available priors are 'gaussian', 'conjugate', 'mixtgauss' and\n",
    "        'conjugate_known_mean'. For each prior, you must\n",
    "        specify the corresponding parameter:\n",
    "          - For the gaussian prior, no parameter is required.\n",
    "          - For the conjugate prior, you must specify\n",
    "            - n_mc_samples, the number of samples used in the Monte Carlo\n",
    "              estimation of the prior loss and its gradient.\n",
    "            - mu_0, the prior sample mean\n",
    "            - kappa_0, the number of samples used to estimate the\n",
    "              prior sample mean\n",
    "            - alpha_0 and beta_0, such that variance was estimated from 2\n",
    "             alpha_0 observations with sample mean mu_0 and sum of squared\n",
    "             deviations 2 beta_0\n",
    "          - For the conjugate prior with known mean,\n",
    "            - n_mc_samples, the number of samples used in the Monte Carlo\n",
    "              estimation of the prior loss and its gradient.\n",
    "            - mean, the known mean\n",
    "            - alpha_0 and beta_0 defined as above\n",
    "          - For the mixture of two gaussians,\n",
    "            - n_mc_samples, the number of samples used in the Monte Carlo\n",
    "              estimation of the prior loss and its gradient.\n",
    "            - sigma_1 and sigma_2 the std of the two gaussians\n",
    "            - pi the probability of the first gaussian\n",
    "        For further information, see:\n",
    "            https://en.wikipedia.org/wiki/Conjugate_prior.\n",
    "        Acts inplace by modifying the value of _prior_loss_function\n",
    "        :args prior_type: one of 'gaussian', 'conjugate',\n",
    "            'conjugate_known_mean', 'mixtgauss'\n",
    "        :args prior_parameters: the parameters for the associated prior\n",
    "        \"\"\"\n",
    "        if prior_type == 'gaussian':\n",
    "            self._prior_loss_function = functools.partial(\n",
    "                sub_prior_loss,\n",
    "                dico=self.dico)\n",
    "        else:\n",
    "            n_mc_samples = prior_parameters.pop(\"n_mc_samples\")\n",
    "            if prior_type == 'conjugate':\n",
    "                mc_logprior_function = functools.partial(\n",
    "                    sub_conjprior,\n",
    "                    **prior_parameters\n",
    "                )\n",
    "            if prior_type == 'conjugate_known_mean':\n",
    "                mc_logprior_function = functools.partial(\n",
    "                    sub_conjpriorknownmean,\n",
    "                    **prior_parameters\n",
    "                )\n",
    "            if prior_type == 'mixtgauss':\n",
    "                mc_logprior_function = functools.partial(\n",
    "                    sub_mixtgaussprior,\n",
    "                    **prior_parameters\n",
    "                )\n",
    "\n",
    "            def prior_loss_function():\n",
    "                \"\"\"Compute the prior loss\"\"\"\n",
    "                logprior = 0.\n",
    "                for _ in range(n_mc_samples):\n",
    "                    rebuild_parameters(\n",
    "                        self.dico, self.model,\n",
    "                        lambda name, p: p.eps.data.normal_()\n",
    "                    )\n",
    "                    logprior += mc_logprior_function(self.dico)\n",
    "                logprior = logprior / n_mc_samples\n",
    "                H = sub_entropy(self.dico)\n",
    "                prior_loss = - logprior - H\n",
    "                return prior_loss\n",
    "            self._prior_loss_function = prior_loss_function\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        def _epsilon_setting(name, p):  # pylint: disable=unused-argument\n",
    "            if self.training:\n",
    "                return p.eps.data.normal_()\n",
    "            return p.eps.data.zero_()\n",
    "\n",
    "        rebuild_parameters(self.dico, self.model, _epsilon_setting)\n",
    "        return self.model(*inputs)\n",
    "\n",
    "    def prior_loss(self):\n",
    "        \"\"\" Returns the prior loss \"\"\"\n",
    "        return self._prior_loss_function()\n",
    "\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    \"\"\" Utility to sample a single model from a Variational Model.\n",
    "    Sample is a decorator that wraps a variational model, sample\n",
    "    a model from the current parameter distribution and make the\n",
    "    model usable as any other pytorch model. The sample can be\n",
    "    redrawn using the draw() method. Draw needs to be called\n",
    "    once before the model can be used.\n",
    "    :args var_model: Variational model from which the sample models\n",
    "    are to be drawn\n",
    "    \"\"\"\n",
    "    def __init__(self, var_model):\n",
    "        super().__init__()\n",
    "        self.var_model = var_model\n",
    "\n",
    "        self.association = []\n",
    "\n",
    "    def draw(self, association=None, var_dico=None):\n",
    "        \"\"\" Draw a single model from the posterior variationally learned \"\"\"\n",
    "        if association is None:\n",
    "            self.association = []\n",
    "            association = self.association\n",
    "            var_dico = self.var_model.dico\n",
    "\n",
    "        for name, p in var_dico.items():\n",
    "            if isinstance(p, VariationalParameter):\n",
    "                if p.eps is None:\n",
    "                    var_dico[name] = p._replace(eps=Variable(\n",
    "                        p.mean.data.clone()))\n",
    "                association.append((var_dico[name].eps,\n",
    "                                    var_dico[name].eps.data.clone().normal_()))\n",
    "            else:\n",
    "                self.draw(association, p)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        for p, drawn_value_p in self.association:\n",
    "            p.data.copy_(drawn_value_p)\n",
    "\n",
    "        def _epsilon_setting(name, p):  # pylint: disable=unused-argument\n",
    "            return 1\n",
    "        rebuild_parameters(self.var_model.dico, self.var_model.model,\n",
    "                           _epsilon_setting)\n",
    "        return self.var_model.model(*inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG11')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.303976\n",
      "Train Epoch: 1 [64/50000 (0%)]\tLoss: 2.316480\n",
      "Train Epoch: 1 [128/50000 (0%)]\tLoss: 2.323314\n",
      "Train Epoch: 1 [192/50000 (0%)]\tLoss: 2.329292\n",
      "Train Epoch: 1 [256/50000 (1%)]\tLoss: 2.342594\n",
      "Train Epoch: 1 [320/50000 (1%)]\tLoss: 2.333108\n",
      "Train Epoch: 1 [384/50000 (1%)]\tLoss: 2.332646\n",
      "Train Epoch: 1 [448/50000 (1%)]\tLoss: 2.346724\n",
      "Train Epoch: 1 [512/50000 (1%)]\tLoss: 2.369165\n",
      "Train Epoch: 1 [576/50000 (1%)]\tLoss: 2.330899\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 2.332583\n",
      "Train Epoch: 1 [704/50000 (1%)]\tLoss: 2.333390\n",
      "Train Epoch: 1 [768/50000 (2%)]\tLoss: 2.359863\n",
      "Train Epoch: 1 [832/50000 (2%)]\tLoss: 2.350430\n",
      "Train Epoch: 1 [896/50000 (2%)]\tLoss: 2.343507\n",
      "Train Epoch: 1 [960/50000 (2%)]\tLoss: 2.346142\n",
      "Train Epoch: 1 [1024/50000 (2%)]\tLoss: 2.355162\n",
      "Train Epoch: 1 [1088/50000 (2%)]\tLoss: 2.349666\n",
      "Train Epoch: 1 [1152/50000 (2%)]\tLoss: 2.343623\n",
      "Train Epoch: 1 [1216/50000 (2%)]\tLoss: 2.341741\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.324555\n",
      "Train Epoch: 1 [1344/50000 (3%)]\tLoss: 2.378093\n",
      "Train Epoch: 1 [1408/50000 (3%)]\tLoss: 2.319991\n",
      "Train Epoch: 1 [1472/50000 (3%)]\tLoss: 2.353105\n",
      "Train Epoch: 1 [1536/50000 (3%)]\tLoss: 2.381108\n",
      "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 2.349151\n",
      "Train Epoch: 1 [1664/50000 (3%)]\tLoss: 2.350765\n",
      "Train Epoch: 1 [1728/50000 (3%)]\tLoss: 2.331651\n",
      "Train Epoch: 1 [1792/50000 (4%)]\tLoss: 2.327917\n",
      "Train Epoch: 1 [1856/50000 (4%)]\tLoss: 2.355311\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 2.350104\n",
      "Train Epoch: 1 [1984/50000 (4%)]\tLoss: 2.358964\n",
      "Train Epoch: 1 [2048/50000 (4%)]\tLoss: 2.369734\n",
      "Train Epoch: 1 [2112/50000 (4%)]\tLoss: 2.384745\n",
      "Train Epoch: 1 [2176/50000 (4%)]\tLoss: 2.363883\n",
      "Train Epoch: 1 [2240/50000 (4%)]\tLoss: 2.340280\n",
      "Train Epoch: 1 [2304/50000 (5%)]\tLoss: 2.358822\n",
      "Train Epoch: 1 [2368/50000 (5%)]\tLoss: 2.345380\n",
      "Train Epoch: 1 [2432/50000 (5%)]\tLoss: 2.357766\n",
      "Train Epoch: 1 [2496/50000 (5%)]\tLoss: 2.357224\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.336169\n",
      "Train Epoch: 1 [2624/50000 (5%)]\tLoss: 2.273770\n",
      "Train Epoch: 1 [2688/50000 (5%)]\tLoss: 2.345421\n",
      "Train Epoch: 1 [2752/50000 (5%)]\tLoss: 2.326927\n",
      "Train Epoch: 1 [2816/50000 (6%)]\tLoss: 2.400467\n",
      "Train Epoch: 1 [2880/50000 (6%)]\tLoss: 2.295156\n",
      "Train Epoch: 1 [2944/50000 (6%)]\tLoss: 2.248408\n",
      "Train Epoch: 1 [3008/50000 (6%)]\tLoss: 2.268570\n",
      "Train Epoch: 1 [3072/50000 (6%)]\tLoss: 2.218224\n",
      "Train Epoch: 1 [3136/50000 (6%)]\tLoss: 2.282264\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.203124\n",
      "Train Epoch: 1 [3264/50000 (7%)]\tLoss: 2.381494\n",
      "Train Epoch: 1 [3328/50000 (7%)]\tLoss: 2.313572\n",
      "Train Epoch: 1 [3392/50000 (7%)]\tLoss: 2.287602\n",
      "Train Epoch: 1 [3456/50000 (7%)]\tLoss: 2.275861\n",
      "Train Epoch: 1 [3520/50000 (7%)]\tLoss: 2.425424\n",
      "Train Epoch: 1 [3584/50000 (7%)]\tLoss: 2.173462\n",
      "Train Epoch: 1 [3648/50000 (7%)]\tLoss: 2.199554\n",
      "Train Epoch: 1 [3712/50000 (7%)]\tLoss: 2.143005\n",
      "Train Epoch: 1 [3776/50000 (8%)]\tLoss: 2.190903\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.119787\n",
      "Train Epoch: 1 [3904/50000 (8%)]\tLoss: 2.391442\n",
      "Train Epoch: 1 [3968/50000 (8%)]\tLoss: 2.403326\n",
      "Train Epoch: 1 [4032/50000 (8%)]\tLoss: 2.226708\n",
      "Train Epoch: 1 [4096/50000 (8%)]\tLoss: 2.119045\n",
      "Train Epoch: 1 [4160/50000 (8%)]\tLoss: 2.427397\n",
      "Train Epoch: 1 [4224/50000 (8%)]\tLoss: 2.619522\n",
      "Train Epoch: 1 [4288/50000 (9%)]\tLoss: 2.269513\n",
      "Train Epoch: 1 [4352/50000 (9%)]\tLoss: 2.256709\n",
      "Train Epoch: 1 [4416/50000 (9%)]\tLoss: 2.261561\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 2.213573\n",
      "Train Epoch: 1 [4544/50000 (9%)]\tLoss: 2.261279\n",
      "Train Epoch: 1 [4608/50000 (9%)]\tLoss: 2.300490\n",
      "Train Epoch: 1 [4672/50000 (9%)]\tLoss: 2.267747\n",
      "Train Epoch: 1 [4736/50000 (9%)]\tLoss: 2.143726\n",
      "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 2.101285\n",
      "Train Epoch: 1 [4864/50000 (10%)]\tLoss: 2.297735\n",
      "Train Epoch: 1 [4928/50000 (10%)]\tLoss: 2.192282\n",
      "Train Epoch: 1 [4992/50000 (10%)]\tLoss: 2.283420\n",
      "Train Epoch: 1 [5056/50000 (10%)]\tLoss: 2.169551\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.220703\n",
      "Train Epoch: 1 [5184/50000 (10%)]\tLoss: 2.274052\n",
      "Train Epoch: 1 [5248/50000 (10%)]\tLoss: 2.136110\n",
      "Train Epoch: 1 [5312/50000 (11%)]\tLoss: 2.188445\n",
      "Train Epoch: 1 [5376/50000 (11%)]\tLoss: 2.245064\n",
      "Train Epoch: 1 [5440/50000 (11%)]\tLoss: 2.218994\n",
      "Train Epoch: 1 [5504/50000 (11%)]\tLoss: 2.291940\n",
      "Train Epoch: 1 [5568/50000 (11%)]\tLoss: 2.201903\n",
      "Train Epoch: 1 [5632/50000 (11%)]\tLoss: 2.158047\n",
      "Train Epoch: 1 [5696/50000 (11%)]\tLoss: 2.361294\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 2.290581\n",
      "Train Epoch: 1 [5824/50000 (12%)]\tLoss: 2.263200\n",
      "Train Epoch: 1 [5888/50000 (12%)]\tLoss: 2.173287\n",
      "Train Epoch: 1 [5952/50000 (12%)]\tLoss: 2.178316\n",
      "Train Epoch: 1 [6016/50000 (12%)]\tLoss: 2.157153\n",
      "Train Epoch: 1 [6080/50000 (12%)]\tLoss: 2.159162\n",
      "Train Epoch: 1 [6144/50000 (12%)]\tLoss: 2.151529\n",
      "Train Epoch: 1 [6208/50000 (12%)]\tLoss: 2.238390\n",
      "Train Epoch: 1 [6272/50000 (13%)]\tLoss: 2.125174\n",
      "Train Epoch: 1 [6336/50000 (13%)]\tLoss: 2.201599\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.073624\n",
      "Train Epoch: 1 [6464/50000 (13%)]\tLoss: 2.156832\n",
      "Train Epoch: 1 [6528/50000 (13%)]\tLoss: 2.386324\n",
      "Train Epoch: 1 [6592/50000 (13%)]\tLoss: 2.181342\n",
      "Train Epoch: 1 [6656/50000 (13%)]\tLoss: 2.165827\n",
      "Train Epoch: 1 [6720/50000 (13%)]\tLoss: 2.014408\n",
      "Train Epoch: 1 [6784/50000 (14%)]\tLoss: 2.206330\n",
      "Train Epoch: 1 [6848/50000 (14%)]\tLoss: 2.242422\n",
      "Train Epoch: 1 [6912/50000 (14%)]\tLoss: 2.326449\n",
      "Train Epoch: 1 [6976/50000 (14%)]\tLoss: 2.195617\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 2.258680\n",
      "Train Epoch: 1 [7104/50000 (14%)]\tLoss: 2.155252\n",
      "Train Epoch: 1 [7168/50000 (14%)]\tLoss: 2.153017\n",
      "Train Epoch: 1 [7232/50000 (14%)]\tLoss: 2.313282\n",
      "Train Epoch: 1 [7296/50000 (15%)]\tLoss: 2.198245\n",
      "Train Epoch: 1 [7360/50000 (15%)]\tLoss: 2.223659\n",
      "Train Epoch: 1 [7424/50000 (15%)]\tLoss: 2.146727\n",
      "Train Epoch: 1 [7488/50000 (15%)]\tLoss: 2.201733\n",
      "Train Epoch: 1 [7552/50000 (15%)]\tLoss: 2.164582\n",
      "Train Epoch: 1 [7616/50000 (15%)]\tLoss: 2.117396\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.007004\n",
      "Train Epoch: 1 [7744/50000 (15%)]\tLoss: 2.024014\n",
      "Train Epoch: 1 [7808/50000 (16%)]\tLoss: 2.209080\n",
      "Train Epoch: 1 [7872/50000 (16%)]\tLoss: 2.328006\n",
      "Train Epoch: 1 [7936/50000 (16%)]\tLoss: 2.312594\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.085536\n",
      "Train Epoch: 1 [8064/50000 (16%)]\tLoss: 2.147480\n",
      "Train Epoch: 1 [8128/50000 (16%)]\tLoss: 2.260565\n",
      "Train Epoch: 1 [8192/50000 (16%)]\tLoss: 2.168098\n",
      "Train Epoch: 1 [8256/50000 (16%)]\tLoss: 2.205589\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 2.112292\n",
      "Train Epoch: 1 [8384/50000 (17%)]\tLoss: 2.105431\n",
      "Train Epoch: 1 [8448/50000 (17%)]\tLoss: 2.046908\n",
      "Train Epoch: 1 [8512/50000 (17%)]\tLoss: 2.056392\n",
      "Train Epoch: 1 [8576/50000 (17%)]\tLoss: 2.211402\n",
      "Train Epoch: 1 [8640/50000 (17%)]\tLoss: 2.266983\n",
      "Train Epoch: 1 [8704/50000 (17%)]\tLoss: 2.282725\n",
      "Train Epoch: 1 [8768/50000 (18%)]\tLoss: 2.149904\n",
      "Train Epoch: 1 [8832/50000 (18%)]\tLoss: 2.124913\n",
      "Train Epoch: 1 [8896/50000 (18%)]\tLoss: 2.213333\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.165925\n",
      "Train Epoch: 1 [9024/50000 (18%)]\tLoss: 2.128543\n",
      "Train Epoch: 1 [9088/50000 (18%)]\tLoss: 2.053321\n",
      "Train Epoch: 1 [9152/50000 (18%)]\tLoss: 2.118074\n",
      "Train Epoch: 1 [9216/50000 (18%)]\tLoss: 2.195527\n",
      "Train Epoch: 1 [9280/50000 (19%)]\tLoss: 2.014701\n",
      "Train Epoch: 1 [9344/50000 (19%)]\tLoss: 2.202586\n",
      "Train Epoch: 1 [9408/50000 (19%)]\tLoss: 2.139652\n",
      "Train Epoch: 1 [9472/50000 (19%)]\tLoss: 2.177702\n",
      "Train Epoch: 1 [9536/50000 (19%)]\tLoss: 2.138592\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.083040\n",
      "Train Epoch: 1 [9664/50000 (19%)]\tLoss: 2.259725\n",
      "Train Epoch: 1 [9728/50000 (19%)]\tLoss: 2.121518\n",
      "Train Epoch: 1 [9792/50000 (20%)]\tLoss: 2.015101\n",
      "Train Epoch: 1 [9856/50000 (20%)]\tLoss: 2.194548\n",
      "Train Epoch: 1 [9920/50000 (20%)]\tLoss: 2.308226\n",
      "Train Epoch: 1 [9984/50000 (20%)]\tLoss: 2.258789\n",
      "Train Epoch: 1 [10048/50000 (20%)]\tLoss: 2.123907\n",
      "Train Epoch: 1 [10112/50000 (20%)]\tLoss: 2.134022\n",
      "Train Epoch: 1 [10176/50000 (20%)]\tLoss: 2.217582\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.268291\n",
      "Train Epoch: 1 [10304/50000 (21%)]\tLoss: 2.164989\n",
      "Train Epoch: 1 [10368/50000 (21%)]\tLoss: 2.137222\n",
      "Train Epoch: 1 [10432/50000 (21%)]\tLoss: 2.301215\n",
      "Train Epoch: 1 [10496/50000 (21%)]\tLoss: 2.252228\n",
      "Train Epoch: 1 [10560/50000 (21%)]\tLoss: 2.112088\n",
      "Train Epoch: 1 [10624/50000 (21%)]\tLoss: 2.159745\n",
      "Train Epoch: 1 [10688/50000 (21%)]\tLoss: 2.143104\n",
      "Train Epoch: 1 [10752/50000 (21%)]\tLoss: 2.134466\n",
      "Train Epoch: 1 [10816/50000 (22%)]\tLoss: 2.237409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 2.143812\n",
      "Train Epoch: 1 [10944/50000 (22%)]\tLoss: 2.230412\n",
      "Train Epoch: 1 [11008/50000 (22%)]\tLoss: 2.150532\n",
      "Train Epoch: 1 [11072/50000 (22%)]\tLoss: 2.187281\n",
      "Train Epoch: 1 [11136/50000 (22%)]\tLoss: 2.109945\n",
      "Train Epoch: 1 [11200/50000 (22%)]\tLoss: 2.226933\n",
      "Train Epoch: 1 [11264/50000 (23%)]\tLoss: 2.173051\n",
      "Train Epoch: 1 [11328/50000 (23%)]\tLoss: 2.202100\n",
      "Train Epoch: 1 [11392/50000 (23%)]\tLoss: 2.174973\n",
      "Train Epoch: 1 [11456/50000 (23%)]\tLoss: 2.138718\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 2.106508\n",
      "Train Epoch: 1 [11584/50000 (23%)]\tLoss: 2.110156\n",
      "Train Epoch: 1 [11648/50000 (23%)]\tLoss: 2.199883\n",
      "Train Epoch: 1 [11712/50000 (23%)]\tLoss: 2.217257\n",
      "Train Epoch: 1 [11776/50000 (24%)]\tLoss: 2.173420\n",
      "Train Epoch: 1 [11840/50000 (24%)]\tLoss: 2.111608\n",
      "Train Epoch: 1 [11904/50000 (24%)]\tLoss: 2.138960\n",
      "Train Epoch: 1 [11968/50000 (24%)]\tLoss: 2.055942\n",
      "Train Epoch: 1 [12032/50000 (24%)]\tLoss: 2.014389\n",
      "Train Epoch: 1 [12096/50000 (24%)]\tLoss: 2.000706\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 2.056978\n",
      "Train Epoch: 1 [12224/50000 (24%)]\tLoss: 2.140208\n",
      "Train Epoch: 1 [12288/50000 (25%)]\tLoss: 2.274399\n",
      "Train Epoch: 1 [12352/50000 (25%)]\tLoss: 2.239890\n",
      "Train Epoch: 1 [12416/50000 (25%)]\tLoss: 2.280470\n",
      "Train Epoch: 1 [12480/50000 (25%)]\tLoss: 2.168695\n",
      "Train Epoch: 1 [12544/50000 (25%)]\tLoss: 2.101128\n",
      "Train Epoch: 1 [12608/50000 (25%)]\tLoss: 2.234467\n",
      "Train Epoch: 1 [12672/50000 (25%)]\tLoss: 2.110444\n",
      "Train Epoch: 1 [12736/50000 (25%)]\tLoss: 2.147355\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.038410\n",
      "Train Epoch: 1 [12864/50000 (26%)]\tLoss: 2.133851\n",
      "Train Epoch: 1 [12928/50000 (26%)]\tLoss: 2.135788\n",
      "Train Epoch: 1 [12992/50000 (26%)]\tLoss: 2.075236\n",
      "Train Epoch: 1 [13056/50000 (26%)]\tLoss: 2.063345\n",
      "Train Epoch: 1 [13120/50000 (26%)]\tLoss: 2.263222\n",
      "Train Epoch: 1 [13184/50000 (26%)]\tLoss: 2.140999\n",
      "Train Epoch: 1 [13248/50000 (26%)]\tLoss: 2.191090\n",
      "Train Epoch: 1 [13312/50000 (27%)]\tLoss: 2.159468\n",
      "Train Epoch: 1 [13376/50000 (27%)]\tLoss: 2.107978\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 2.066716\n",
      "Train Epoch: 1 [13504/50000 (27%)]\tLoss: 2.125317\n",
      "Train Epoch: 1 [13568/50000 (27%)]\tLoss: 2.031564\n",
      "Train Epoch: 1 [13632/50000 (27%)]\tLoss: 2.119273\n",
      "Train Epoch: 1 [13696/50000 (27%)]\tLoss: 2.036586\n",
      "Train Epoch: 1 [13760/50000 (27%)]\tLoss: 2.136526\n",
      "Train Epoch: 1 [13824/50000 (28%)]\tLoss: 2.374293\n",
      "Train Epoch: 1 [13888/50000 (28%)]\tLoss: 2.151840\n",
      "Train Epoch: 1 [13952/50000 (28%)]\tLoss: 2.078343\n",
      "Train Epoch: 1 [14016/50000 (28%)]\tLoss: 2.149852\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 2.164981\n",
      "Train Epoch: 1 [14144/50000 (28%)]\tLoss: 2.027555\n",
      "Train Epoch: 1 [14208/50000 (28%)]\tLoss: 2.139520\n",
      "Train Epoch: 1 [14272/50000 (29%)]\tLoss: 2.106075\n",
      "Train Epoch: 1 [14336/50000 (29%)]\tLoss: 2.175503\n",
      "Train Epoch: 1 [14400/50000 (29%)]\tLoss: 2.181713\n",
      "Train Epoch: 1 [14464/50000 (29%)]\tLoss: 2.125200\n",
      "Train Epoch: 1 [14528/50000 (29%)]\tLoss: 2.204938\n",
      "Train Epoch: 1 [14592/50000 (29%)]\tLoss: 2.276483\n",
      "Train Epoch: 1 [14656/50000 (29%)]\tLoss: 2.133968\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 2.167175\n",
      "Train Epoch: 1 [14784/50000 (30%)]\tLoss: 2.079164\n",
      "Train Epoch: 1 [14848/50000 (30%)]\tLoss: 2.277124\n",
      "Train Epoch: 1 [14912/50000 (30%)]\tLoss: 2.141404\n",
      "Train Epoch: 1 [14976/50000 (30%)]\tLoss: 2.106148\n",
      "Train Epoch: 1 [15040/50000 (30%)]\tLoss: 2.127404\n",
      "Train Epoch: 1 [15104/50000 (30%)]\tLoss: 2.085352\n",
      "Train Epoch: 1 [15168/50000 (30%)]\tLoss: 2.093236\n",
      "Train Epoch: 1 [15232/50000 (30%)]\tLoss: 2.058295\n",
      "Train Epoch: 1 [15296/50000 (31%)]\tLoss: 1.968783\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 2.090729\n",
      "Train Epoch: 1 [15424/50000 (31%)]\tLoss: 2.278903\n",
      "Train Epoch: 1 [15488/50000 (31%)]\tLoss: 2.127748\n",
      "Train Epoch: 1 [15552/50000 (31%)]\tLoss: 2.006091\n",
      "Train Epoch: 1 [15616/50000 (31%)]\tLoss: 2.165267\n",
      "Train Epoch: 1 [15680/50000 (31%)]\tLoss: 2.252847\n",
      "Train Epoch: 1 [15744/50000 (31%)]\tLoss: 2.136005\n",
      "Train Epoch: 1 [15808/50000 (32%)]\tLoss: 2.336918\n",
      "Train Epoch: 1 [15872/50000 (32%)]\tLoss: 2.314562\n",
      "Train Epoch: 1 [15936/50000 (32%)]\tLoss: 2.154336\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.249434\n",
      "Train Epoch: 1 [16064/50000 (32%)]\tLoss: 2.156064\n",
      "Train Epoch: 1 [16128/50000 (32%)]\tLoss: 2.163646\n",
      "Train Epoch: 1 [16192/50000 (32%)]\tLoss: 2.083752\n",
      "Train Epoch: 1 [16256/50000 (32%)]\tLoss: 2.131936\n",
      "Train Epoch: 1 [16320/50000 (33%)]\tLoss: 1.990914\n",
      "Train Epoch: 1 [16384/50000 (33%)]\tLoss: 2.158423\n",
      "Train Epoch: 1 [16448/50000 (33%)]\tLoss: 2.073322\n",
      "Train Epoch: 1 [16512/50000 (33%)]\tLoss: 2.192958\n",
      "Train Epoch: 1 [16576/50000 (33%)]\tLoss: 2.119854\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 2.215877\n",
      "Train Epoch: 1 [16704/50000 (33%)]\tLoss: 2.325419\n",
      "Train Epoch: 1 [16768/50000 (34%)]\tLoss: 2.339732\n",
      "Train Epoch: 1 [16832/50000 (34%)]\tLoss: 2.054252\n",
      "Train Epoch: 1 [16896/50000 (34%)]\tLoss: 2.132761\n",
      "Train Epoch: 1 [16960/50000 (34%)]\tLoss: 1.993600\n",
      "Train Epoch: 1 [17024/50000 (34%)]\tLoss: 2.111712\n",
      "Train Epoch: 1 [17088/50000 (34%)]\tLoss: 2.055667\n",
      "Train Epoch: 1 [17152/50000 (34%)]\tLoss: 2.241319\n",
      "Train Epoch: 1 [17216/50000 (34%)]\tLoss: 2.082084\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 2.049186\n",
      "Train Epoch: 1 [17344/50000 (35%)]\tLoss: 2.109302\n",
      "Train Epoch: 1 [17408/50000 (35%)]\tLoss: 2.096468\n",
      "Train Epoch: 1 [17472/50000 (35%)]\tLoss: 2.284042\n",
      "Train Epoch: 1 [17536/50000 (35%)]\tLoss: 2.159031\n",
      "Train Epoch: 1 [17600/50000 (35%)]\tLoss: 2.128406\n",
      "Train Epoch: 1 [17664/50000 (35%)]\tLoss: 2.190928\n",
      "Train Epoch: 1 [17728/50000 (35%)]\tLoss: 2.073395\n",
      "Train Epoch: 1 [17792/50000 (36%)]\tLoss: 1.992322\n",
      "Train Epoch: 1 [17856/50000 (36%)]\tLoss: 2.176732\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 2.259099\n",
      "Train Epoch: 1 [17984/50000 (36%)]\tLoss: 2.043290\n",
      "Train Epoch: 1 [18048/50000 (36%)]\tLoss: 2.018469\n",
      "Train Epoch: 1 [18112/50000 (36%)]\tLoss: 2.046659\n",
      "Train Epoch: 1 [18176/50000 (36%)]\tLoss: 2.032281\n",
      "Train Epoch: 1 [18240/50000 (36%)]\tLoss: 2.046586\n",
      "Train Epoch: 1 [18304/50000 (37%)]\tLoss: 2.193940\n",
      "Train Epoch: 1 [18368/50000 (37%)]\tLoss: 2.035988\n",
      "Train Epoch: 1 [18432/50000 (37%)]\tLoss: 2.151797\n",
      "Train Epoch: 1 [18496/50000 (37%)]\tLoss: 2.183444\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 2.056395\n",
      "Train Epoch: 1 [18624/50000 (37%)]\tLoss: 2.157828\n",
      "Train Epoch: 1 [18688/50000 (37%)]\tLoss: 2.117294\n",
      "Train Epoch: 1 [18752/50000 (37%)]\tLoss: 2.180391\n",
      "Train Epoch: 1 [18816/50000 (38%)]\tLoss: 2.107563\n",
      "Train Epoch: 1 [18880/50000 (38%)]\tLoss: 1.999548\n",
      "Train Epoch: 1 [18944/50000 (38%)]\tLoss: 2.226487\n",
      "Train Epoch: 1 [19008/50000 (38%)]\tLoss: 2.181346\n",
      "Train Epoch: 1 [19072/50000 (38%)]\tLoss: 2.195184\n",
      "Train Epoch: 1 [19136/50000 (38%)]\tLoss: 2.083334\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.174393\n",
      "Train Epoch: 1 [19264/50000 (38%)]\tLoss: 2.070218\n",
      "Train Epoch: 1 [19328/50000 (39%)]\tLoss: 2.208305\n",
      "Train Epoch: 1 [19392/50000 (39%)]\tLoss: 2.112796\n",
      "Train Epoch: 1 [19456/50000 (39%)]\tLoss: 2.142441\n",
      "Train Epoch: 1 [19520/50000 (39%)]\tLoss: 2.070054\n",
      "Train Epoch: 1 [19584/50000 (39%)]\tLoss: 2.157326\n",
      "Train Epoch: 1 [19648/50000 (39%)]\tLoss: 2.043482\n",
      "Train Epoch: 1 [19712/50000 (39%)]\tLoss: 2.071523\n",
      "Train Epoch: 1 [19776/50000 (40%)]\tLoss: 2.039146\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 2.104312\n",
      "Train Epoch: 1 [19904/50000 (40%)]\tLoss: 2.121887\n",
      "Train Epoch: 1 [19968/50000 (40%)]\tLoss: 1.910349\n",
      "Train Epoch: 1 [20032/50000 (40%)]\tLoss: 2.141912\n",
      "Train Epoch: 1 [20096/50000 (40%)]\tLoss: 2.142559\n",
      "Train Epoch: 1 [20160/50000 (40%)]\tLoss: 2.287276\n",
      "Train Epoch: 1 [20224/50000 (40%)]\tLoss: 2.139504\n",
      "Train Epoch: 1 [20288/50000 (41%)]\tLoss: 2.021255\n",
      "Train Epoch: 1 [20352/50000 (41%)]\tLoss: 2.209235\n",
      "Train Epoch: 1 [20416/50000 (41%)]\tLoss: 2.029162\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 2.041271\n",
      "Train Epoch: 1 [20544/50000 (41%)]\tLoss: 2.102567\n",
      "Train Epoch: 1 [20608/50000 (41%)]\tLoss: 2.259613\n",
      "Train Epoch: 1 [20672/50000 (41%)]\tLoss: 2.194670\n",
      "Train Epoch: 1 [20736/50000 (41%)]\tLoss: 2.034703\n",
      "Train Epoch: 1 [20800/50000 (42%)]\tLoss: 2.074389\n",
      "Train Epoch: 1 [20864/50000 (42%)]\tLoss: 2.025038\n",
      "Train Epoch: 1 [20928/50000 (42%)]\tLoss: 2.073676\n",
      "Train Epoch: 1 [20992/50000 (42%)]\tLoss: 2.104698\n",
      "Train Epoch: 1 [21056/50000 (42%)]\tLoss: 2.169130\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 1.914822\n",
      "Train Epoch: 1 [21184/50000 (42%)]\tLoss: 2.175006\n",
      "Train Epoch: 1 [21248/50000 (42%)]\tLoss: 2.271258\n",
      "Train Epoch: 1 [21312/50000 (43%)]\tLoss: 1.971262\n",
      "Train Epoch: 1 [21376/50000 (43%)]\tLoss: 2.021163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [21440/50000 (43%)]\tLoss: 1.944796\n",
      "Train Epoch: 1 [21504/50000 (43%)]\tLoss: 2.178795\n",
      "Train Epoch: 1 [21568/50000 (43%)]\tLoss: 2.022480\n",
      "Train Epoch: 1 [21632/50000 (43%)]\tLoss: 2.227464\n",
      "Train Epoch: 1 [21696/50000 (43%)]\tLoss: 2.117456\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 2.083572\n",
      "Train Epoch: 1 [21824/50000 (44%)]\tLoss: 1.993006\n",
      "Train Epoch: 1 [21888/50000 (44%)]\tLoss: 1.993093\n",
      "Train Epoch: 1 [21952/50000 (44%)]\tLoss: 2.232445\n",
      "Train Epoch: 1 [22016/50000 (44%)]\tLoss: 2.168397\n",
      "Train Epoch: 1 [22080/50000 (44%)]\tLoss: 2.112290\n",
      "Train Epoch: 1 [22144/50000 (44%)]\tLoss: 1.963454\n",
      "Train Epoch: 1 [22208/50000 (44%)]\tLoss: 1.931482\n",
      "Train Epoch: 1 [22272/50000 (45%)]\tLoss: 2.075489\n",
      "Train Epoch: 1 [22336/50000 (45%)]\tLoss: 2.118565\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.088529\n",
      "Train Epoch: 1 [22464/50000 (45%)]\tLoss: 1.986331\n",
      "Train Epoch: 1 [22528/50000 (45%)]\tLoss: 2.048859\n",
      "Train Epoch: 1 [22592/50000 (45%)]\tLoss: 2.278417\n",
      "Train Epoch: 1 [22656/50000 (45%)]\tLoss: 1.943944\n",
      "Train Epoch: 1 [22720/50000 (45%)]\tLoss: 2.065111\n",
      "Train Epoch: 1 [22784/50000 (46%)]\tLoss: 2.019978\n",
      "Train Epoch: 1 [22848/50000 (46%)]\tLoss: 2.007634\n",
      "Train Epoch: 1 [22912/50000 (46%)]\tLoss: 2.119562\n",
      "Train Epoch: 1 [22976/50000 (46%)]\tLoss: 2.034811\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.973106\n",
      "Train Epoch: 1 [23104/50000 (46%)]\tLoss: 2.146397\n",
      "Train Epoch: 1 [23168/50000 (46%)]\tLoss: 1.973951\n",
      "Train Epoch: 1 [23232/50000 (46%)]\tLoss: 2.134198\n",
      "Train Epoch: 1 [23296/50000 (47%)]\tLoss: 2.029115\n",
      "Train Epoch: 1 [23360/50000 (47%)]\tLoss: 2.095526\n",
      "Train Epoch: 1 [23424/50000 (47%)]\tLoss: 2.142514\n",
      "Train Epoch: 1 [23488/50000 (47%)]\tLoss: 2.088262\n",
      "Train Epoch: 1 [23552/50000 (47%)]\tLoss: 2.108339\n",
      "Train Epoch: 1 [23616/50000 (47%)]\tLoss: 2.020352\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 2.025540\n",
      "Train Epoch: 1 [23744/50000 (47%)]\tLoss: 2.234786\n",
      "Train Epoch: 1 [23808/50000 (48%)]\tLoss: 2.100806\n",
      "Train Epoch: 1 [23872/50000 (48%)]\tLoss: 2.132661\n",
      "Train Epoch: 1 [23936/50000 (48%)]\tLoss: 2.031126\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 2.025033\n",
      "Train Epoch: 1 [24064/50000 (48%)]\tLoss: 1.968002\n",
      "Train Epoch: 1 [24128/50000 (48%)]\tLoss: 1.960578\n",
      "Train Epoch: 1 [24192/50000 (48%)]\tLoss: 2.115964\n",
      "Train Epoch: 1 [24256/50000 (48%)]\tLoss: 2.101905\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.990898\n",
      "Train Epoch: 1 [24384/50000 (49%)]\tLoss: 2.137455\n",
      "Train Epoch: 1 [24448/50000 (49%)]\tLoss: 2.288471\n",
      "Train Epoch: 1 [24512/50000 (49%)]\tLoss: 2.055117\n",
      "Train Epoch: 1 [24576/50000 (49%)]\tLoss: 1.923884\n",
      "Train Epoch: 1 [24640/50000 (49%)]\tLoss: 2.204389\n",
      "Train Epoch: 1 [24704/50000 (49%)]\tLoss: 2.112161\n",
      "Train Epoch: 1 [24768/50000 (49%)]\tLoss: 2.146919\n",
      "Train Epoch: 1 [24832/50000 (50%)]\tLoss: 2.133166\n",
      "Train Epoch: 1 [24896/50000 (50%)]\tLoss: 2.090585\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 2.085819\n",
      "Train Epoch: 1 [25024/50000 (50%)]\tLoss: 2.107677\n",
      "Train Epoch: 1 [25088/50000 (50%)]\tLoss: 2.175292\n",
      "Train Epoch: 1 [25152/50000 (50%)]\tLoss: 2.051708\n",
      "Train Epoch: 1 [25216/50000 (50%)]\tLoss: 2.005006\n",
      "Train Epoch: 1 [25280/50000 (51%)]\tLoss: 2.151262\n",
      "Train Epoch: 1 [25344/50000 (51%)]\tLoss: 2.066542\n",
      "Train Epoch: 1 [25408/50000 (51%)]\tLoss: 2.066852\n",
      "Train Epoch: 1 [25472/50000 (51%)]\tLoss: 2.295077\n",
      "Train Epoch: 1 [25536/50000 (51%)]\tLoss: 2.088694\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.218522\n",
      "Train Epoch: 1 [25664/50000 (51%)]\tLoss: 2.036579\n",
      "Train Epoch: 1 [25728/50000 (51%)]\tLoss: 2.327773\n",
      "Train Epoch: 1 [25792/50000 (52%)]\tLoss: 2.024032\n",
      "Train Epoch: 1 [25856/50000 (52%)]\tLoss: 2.155361\n",
      "Train Epoch: 1 [25920/50000 (52%)]\tLoss: 2.150535\n",
      "Train Epoch: 1 [25984/50000 (52%)]\tLoss: 2.103137\n",
      "Train Epoch: 1 [26048/50000 (52%)]\tLoss: 2.086030\n",
      "Train Epoch: 1 [26112/50000 (52%)]\tLoss: 2.118440\n",
      "Train Epoch: 1 [26176/50000 (52%)]\tLoss: 2.326244\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 2.269345\n",
      "Train Epoch: 1 [26304/50000 (53%)]\tLoss: 2.039670\n",
      "Train Epoch: 1 [26368/50000 (53%)]\tLoss: 2.078554\n",
      "Train Epoch: 1 [26432/50000 (53%)]\tLoss: 1.979409\n",
      "Train Epoch: 1 [26496/50000 (53%)]\tLoss: 2.086482\n",
      "Train Epoch: 1 [26560/50000 (53%)]\tLoss: 2.209241\n",
      "Train Epoch: 1 [26624/50000 (53%)]\tLoss: 2.014828\n",
      "Train Epoch: 1 [26688/50000 (53%)]\tLoss: 2.036664\n",
      "Train Epoch: 1 [26752/50000 (53%)]\tLoss: 2.030008\n",
      "Train Epoch: 1 [26816/50000 (54%)]\tLoss: 2.174304\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.908974\n",
      "Train Epoch: 1 [26944/50000 (54%)]\tLoss: 1.994619\n",
      "Train Epoch: 1 [27008/50000 (54%)]\tLoss: 1.938016\n",
      "Train Epoch: 1 [27072/50000 (54%)]\tLoss: 2.024886\n",
      "Train Epoch: 1 [27136/50000 (54%)]\tLoss: 2.142493\n",
      "Train Epoch: 1 [27200/50000 (54%)]\tLoss: 2.146281\n",
      "Train Epoch: 1 [27264/50000 (54%)]\tLoss: 2.009135\n",
      "Train Epoch: 1 [27328/50000 (55%)]\tLoss: 2.049989\n",
      "Train Epoch: 1 [27392/50000 (55%)]\tLoss: 1.991175\n",
      "Train Epoch: 1 [27456/50000 (55%)]\tLoss: 2.320138\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 2.109811\n",
      "Train Epoch: 1 [27584/50000 (55%)]\tLoss: 1.994039\n",
      "Train Epoch: 1 [27648/50000 (55%)]\tLoss: 2.021215\n",
      "Train Epoch: 1 [27712/50000 (55%)]\tLoss: 2.245214\n",
      "Train Epoch: 1 [27776/50000 (55%)]\tLoss: 2.052251\n",
      "Train Epoch: 1 [27840/50000 (56%)]\tLoss: 2.020905\n",
      "Train Epoch: 1 [27904/50000 (56%)]\tLoss: 2.116468\n",
      "Train Epoch: 1 [27968/50000 (56%)]\tLoss: 2.210408\n",
      "Train Epoch: 1 [28032/50000 (56%)]\tLoss: 2.225387\n",
      "Train Epoch: 1 [28096/50000 (56%)]\tLoss: 2.183950\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 2.000257\n",
      "Train Epoch: 1 [28224/50000 (56%)]\tLoss: 1.975937\n",
      "Train Epoch: 1 [28288/50000 (57%)]\tLoss: 2.204710\n",
      "Train Epoch: 1 [28352/50000 (57%)]\tLoss: 2.017034\n",
      "Train Epoch: 1 [28416/50000 (57%)]\tLoss: 2.008433\n",
      "Train Epoch: 1 [28480/50000 (57%)]\tLoss: 2.068557\n",
      "Train Epoch: 1 [28544/50000 (57%)]\tLoss: 2.004750\n",
      "Train Epoch: 1 [28608/50000 (57%)]\tLoss: 2.073318\n",
      "Train Epoch: 1 [28672/50000 (57%)]\tLoss: 1.964427\n",
      "Train Epoch: 1 [28736/50000 (57%)]\tLoss: 1.995201\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.053040\n",
      "Train Epoch: 1 [28864/50000 (58%)]\tLoss: 1.986840\n",
      "Train Epoch: 1 [28928/50000 (58%)]\tLoss: 1.950026\n",
      "Train Epoch: 1 [28992/50000 (58%)]\tLoss: 2.084855\n",
      "Train Epoch: 1 [29056/50000 (58%)]\tLoss: 2.138614\n",
      "Train Epoch: 1 [29120/50000 (58%)]\tLoss: 2.113514\n",
      "Train Epoch: 1 [29184/50000 (58%)]\tLoss: 1.936972\n",
      "Train Epoch: 1 [29248/50000 (58%)]\tLoss: 2.001577\n",
      "Train Epoch: 1 [29312/50000 (59%)]\tLoss: 2.081450\n",
      "Train Epoch: 1 [29376/50000 (59%)]\tLoss: 1.956691\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 2.001327\n",
      "Train Epoch: 1 [29504/50000 (59%)]\tLoss: 2.368781\n",
      "Train Epoch: 1 [29568/50000 (59%)]\tLoss: 1.896416\n",
      "Train Epoch: 1 [29632/50000 (59%)]\tLoss: 2.248919\n",
      "Train Epoch: 1 [29696/50000 (59%)]\tLoss: 1.940773\n",
      "Train Epoch: 1 [29760/50000 (59%)]\tLoss: 2.202774\n",
      "Train Epoch: 1 [29824/50000 (60%)]\tLoss: 1.941217\n",
      "Train Epoch: 1 [29888/50000 (60%)]\tLoss: 2.083975\n",
      "Train Epoch: 1 [29952/50000 (60%)]\tLoss: 1.989061\n",
      "Train Epoch: 1 [30016/50000 (60%)]\tLoss: 2.017834\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 2.098433\n",
      "Train Epoch: 1 [30144/50000 (60%)]\tLoss: 2.069368\n",
      "Train Epoch: 1 [30208/50000 (60%)]\tLoss: 2.140636\n",
      "Train Epoch: 1 [30272/50000 (60%)]\tLoss: 2.129006\n",
      "Train Epoch: 1 [30336/50000 (61%)]\tLoss: 2.055581\n",
      "Train Epoch: 1 [30400/50000 (61%)]\tLoss: 2.109195\n",
      "Train Epoch: 1 [30464/50000 (61%)]\tLoss: 2.054923\n",
      "Train Epoch: 1 [30528/50000 (61%)]\tLoss: 2.031347\n",
      "Train Epoch: 1 [30592/50000 (61%)]\tLoss: 2.078293\n",
      "Train Epoch: 1 [30656/50000 (61%)]\tLoss: 2.058597\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 2.130659\n",
      "Train Epoch: 1 [30784/50000 (62%)]\tLoss: 1.907226\n",
      "Train Epoch: 1 [30848/50000 (62%)]\tLoss: 2.116490\n",
      "Train Epoch: 1 [30912/50000 (62%)]\tLoss: 1.960719\n",
      "Train Epoch: 1 [30976/50000 (62%)]\tLoss: 1.947942\n",
      "Train Epoch: 1 [31040/50000 (62%)]\tLoss: 2.066469\n",
      "Train Epoch: 1 [31104/50000 (62%)]\tLoss: 2.011890\n",
      "Train Epoch: 1 [31168/50000 (62%)]\tLoss: 1.962498\n",
      "Train Epoch: 1 [31232/50000 (62%)]\tLoss: 2.105406\n",
      "Train Epoch: 1 [31296/50000 (63%)]\tLoss: 2.068355\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 1.986914\n",
      "Train Epoch: 1 [31424/50000 (63%)]\tLoss: 1.986248\n",
      "Train Epoch: 1 [31488/50000 (63%)]\tLoss: 1.927479\n",
      "Train Epoch: 1 [31552/50000 (63%)]\tLoss: 2.074144\n",
      "Train Epoch: 1 [31616/50000 (63%)]\tLoss: 2.094767\n",
      "Train Epoch: 1 [31680/50000 (63%)]\tLoss: 2.171069\n",
      "Train Epoch: 1 [31744/50000 (63%)]\tLoss: 2.008964\n",
      "Train Epoch: 1 [31808/50000 (64%)]\tLoss: 1.953810\n",
      "Train Epoch: 1 [31872/50000 (64%)]\tLoss: 2.056190\n",
      "Train Epoch: 1 [31936/50000 (64%)]\tLoss: 2.105156\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.030061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32064/50000 (64%)]\tLoss: 1.937306\n",
      "Train Epoch: 1 [32128/50000 (64%)]\tLoss: 2.040097\n",
      "Train Epoch: 1 [32192/50000 (64%)]\tLoss: 2.129510\n",
      "Train Epoch: 1 [32256/50000 (64%)]\tLoss: 1.844652\n",
      "Train Epoch: 1 [32320/50000 (65%)]\tLoss: 2.036918\n",
      "Train Epoch: 1 [32384/50000 (65%)]\tLoss: 1.934390\n",
      "Train Epoch: 1 [32448/50000 (65%)]\tLoss: 2.071781\n",
      "Train Epoch: 1 [32512/50000 (65%)]\tLoss: 2.042880\n",
      "Train Epoch: 1 [32576/50000 (65%)]\tLoss: 1.987012\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 2.073626\n",
      "Train Epoch: 1 [32704/50000 (65%)]\tLoss: 1.945405\n",
      "Train Epoch: 1 [32768/50000 (65%)]\tLoss: 2.060571\n",
      "Train Epoch: 1 [32832/50000 (66%)]\tLoss: 2.035054\n",
      "Train Epoch: 1 [32896/50000 (66%)]\tLoss: 1.954267\n",
      "Train Epoch: 1 [32960/50000 (66%)]\tLoss: 2.045078\n",
      "Train Epoch: 1 [33024/50000 (66%)]\tLoss: 1.906338\n",
      "Train Epoch: 1 [33088/50000 (66%)]\tLoss: 2.228701\n",
      "Train Epoch: 1 [33152/50000 (66%)]\tLoss: 2.070502\n",
      "Train Epoch: 1 [33216/50000 (66%)]\tLoss: 2.064153\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.908803\n",
      "Train Epoch: 1 [33344/50000 (67%)]\tLoss: 1.886622\n",
      "Train Epoch: 1 [33408/50000 (67%)]\tLoss: 1.811792\n",
      "Train Epoch: 1 [33472/50000 (67%)]\tLoss: 2.063444\n",
      "Train Epoch: 1 [33536/50000 (67%)]\tLoss: 2.114982\n",
      "Train Epoch: 1 [33600/50000 (67%)]\tLoss: 2.073833\n",
      "Train Epoch: 1 [33664/50000 (67%)]\tLoss: 2.171675\n",
      "Train Epoch: 1 [33728/50000 (67%)]\tLoss: 1.954511\n",
      "Train Epoch: 1 [33792/50000 (68%)]\tLoss: 2.263249\n",
      "Train Epoch: 1 [33856/50000 (68%)]\tLoss: 2.005159\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 2.010139\n",
      "Train Epoch: 1 [33984/50000 (68%)]\tLoss: 2.110801\n",
      "Train Epoch: 1 [34048/50000 (68%)]\tLoss: 2.043058\n",
      "Train Epoch: 1 [34112/50000 (68%)]\tLoss: 2.052413\n",
      "Train Epoch: 1 [34176/50000 (68%)]\tLoss: 2.059934\n",
      "Train Epoch: 1 [34240/50000 (68%)]\tLoss: 1.938427\n",
      "Train Epoch: 1 [34304/50000 (69%)]\tLoss: 2.123811\n",
      "Train Epoch: 1 [34368/50000 (69%)]\tLoss: 1.966193\n",
      "Train Epoch: 1 [34432/50000 (69%)]\tLoss: 1.879267\n",
      "Train Epoch: 1 [34496/50000 (69%)]\tLoss: 2.113128\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 2.046957\n",
      "Train Epoch: 1 [34624/50000 (69%)]\tLoss: 1.847557\n",
      "Train Epoch: 1 [34688/50000 (69%)]\tLoss: 2.012623\n",
      "Train Epoch: 1 [34752/50000 (69%)]\tLoss: 2.090341\n",
      "Train Epoch: 1 [34816/50000 (70%)]\tLoss: 1.953032\n",
      "Train Epoch: 1 [34880/50000 (70%)]\tLoss: 1.958810\n",
      "Train Epoch: 1 [34944/50000 (70%)]\tLoss: 1.972468\n",
      "Train Epoch: 1 [35008/50000 (70%)]\tLoss: 2.003269\n",
      "Train Epoch: 1 [35072/50000 (70%)]\tLoss: 1.828015\n",
      "Train Epoch: 1 [35136/50000 (70%)]\tLoss: 2.211487\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.134619\n",
      "Train Epoch: 1 [35264/50000 (70%)]\tLoss: 1.953666\n",
      "Train Epoch: 1 [35328/50000 (71%)]\tLoss: 1.864978\n",
      "Train Epoch: 1 [35392/50000 (71%)]\tLoss: 1.883768\n",
      "Train Epoch: 1 [35456/50000 (71%)]\tLoss: 1.808912\n",
      "Train Epoch: 1 [35520/50000 (71%)]\tLoss: 2.173769\n",
      "Train Epoch: 1 [35584/50000 (71%)]\tLoss: 2.114641\n",
      "Train Epoch: 1 [35648/50000 (71%)]\tLoss: 2.029209\n",
      "Train Epoch: 1 [35712/50000 (71%)]\tLoss: 2.072419\n",
      "Train Epoch: 1 [35776/50000 (71%)]\tLoss: 1.878064\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.879656\n",
      "Train Epoch: 1 [35904/50000 (72%)]\tLoss: 1.926690\n",
      "Train Epoch: 1 [35968/50000 (72%)]\tLoss: 2.200148\n",
      "Train Epoch: 1 [36032/50000 (72%)]\tLoss: 2.066308\n",
      "Train Epoch: 1 [36096/50000 (72%)]\tLoss: 2.055126\n",
      "Train Epoch: 1 [36160/50000 (72%)]\tLoss: 1.989448\n",
      "Train Epoch: 1 [36224/50000 (72%)]\tLoss: 2.138898\n",
      "Train Epoch: 1 [36288/50000 (73%)]\tLoss: 1.963443\n",
      "Train Epoch: 1 [36352/50000 (73%)]\tLoss: 2.245618\n",
      "Train Epoch: 1 [36416/50000 (73%)]\tLoss: 1.931177\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 2.020648\n",
      "Train Epoch: 1 [36544/50000 (73%)]\tLoss: 2.139253\n",
      "Train Epoch: 1 [36608/50000 (73%)]\tLoss: 2.162372\n",
      "Train Epoch: 1 [36672/50000 (73%)]\tLoss: 2.211668\n",
      "Train Epoch: 1 [36736/50000 (73%)]\tLoss: 1.951799\n",
      "Train Epoch: 1 [36800/50000 (74%)]\tLoss: 2.294936\n",
      "Train Epoch: 1 [36864/50000 (74%)]\tLoss: 1.999432\n",
      "Train Epoch: 1 [36928/50000 (74%)]\tLoss: 1.940468\n",
      "Train Epoch: 1 [36992/50000 (74%)]\tLoss: 1.958504\n",
      "Train Epoch: 1 [37056/50000 (74%)]\tLoss: 1.836148\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.957419\n",
      "Train Epoch: 1 [37184/50000 (74%)]\tLoss: 2.165365\n",
      "Train Epoch: 1 [37248/50000 (74%)]\tLoss: 2.094665\n",
      "Train Epoch: 1 [37312/50000 (75%)]\tLoss: 2.032155\n",
      "Train Epoch: 1 [37376/50000 (75%)]\tLoss: 2.025375\n",
      "Train Epoch: 1 [37440/50000 (75%)]\tLoss: 2.042211\n",
      "Train Epoch: 1 [37504/50000 (75%)]\tLoss: 2.105887\n",
      "Train Epoch: 1 [37568/50000 (75%)]\tLoss: 2.129953\n",
      "Train Epoch: 1 [37632/50000 (75%)]\tLoss: 1.993048\n",
      "Train Epoch: 1 [37696/50000 (75%)]\tLoss: 2.185127\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 1.941440\n",
      "Train Epoch: 1 [37824/50000 (76%)]\tLoss: 2.009793\n",
      "Train Epoch: 1 [37888/50000 (76%)]\tLoss: 2.103449\n",
      "Train Epoch: 1 [37952/50000 (76%)]\tLoss: 1.919158\n",
      "Train Epoch: 1 [38016/50000 (76%)]\tLoss: 2.105657\n",
      "Train Epoch: 1 [38080/50000 (76%)]\tLoss: 1.947643\n",
      "Train Epoch: 1 [38144/50000 (76%)]\tLoss: 1.922011\n",
      "Train Epoch: 1 [38208/50000 (76%)]\tLoss: 2.026972\n",
      "Train Epoch: 1 [38272/50000 (76%)]\tLoss: 2.093360\n",
      "Train Epoch: 1 [38336/50000 (77%)]\tLoss: 2.208570\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.892566\n",
      "Train Epoch: 1 [38464/50000 (77%)]\tLoss: 1.956988\n",
      "Train Epoch: 1 [38528/50000 (77%)]\tLoss: 1.916451\n",
      "Train Epoch: 1 [38592/50000 (77%)]\tLoss: 2.139966\n",
      "Train Epoch: 1 [38656/50000 (77%)]\tLoss: 1.933719\n",
      "Train Epoch: 1 [38720/50000 (77%)]\tLoss: 2.012070\n",
      "Train Epoch: 1 [38784/50000 (77%)]\tLoss: 2.089229\n",
      "Train Epoch: 1 [38848/50000 (78%)]\tLoss: 1.957612\n",
      "Train Epoch: 1 [38912/50000 (78%)]\tLoss: 2.107073\n",
      "Train Epoch: 1 [38976/50000 (78%)]\tLoss: 1.867513\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 1.946635\n",
      "Train Epoch: 1 [39104/50000 (78%)]\tLoss: 1.931350\n",
      "Train Epoch: 1 [39168/50000 (78%)]\tLoss: 1.966229\n",
      "Train Epoch: 1 [39232/50000 (78%)]\tLoss: 2.169089\n",
      "Train Epoch: 1 [39296/50000 (79%)]\tLoss: 2.013583\n",
      "Train Epoch: 1 [39360/50000 (79%)]\tLoss: 1.991978\n",
      "Train Epoch: 1 [39424/50000 (79%)]\tLoss: 1.852374\n",
      "Train Epoch: 1 [39488/50000 (79%)]\tLoss: 1.898804\n",
      "Train Epoch: 1 [39552/50000 (79%)]\tLoss: 1.915497\n",
      "Train Epoch: 1 [39616/50000 (79%)]\tLoss: 1.743055\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.925749\n",
      "Train Epoch: 1 [39744/50000 (79%)]\tLoss: 2.068462\n",
      "Train Epoch: 1 [39808/50000 (80%)]\tLoss: 1.955643\n",
      "Train Epoch: 1 [39872/50000 (80%)]\tLoss: 2.294605\n",
      "Train Epoch: 1 [39936/50000 (80%)]\tLoss: 2.160640\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.996603\n",
      "Train Epoch: 1 [40064/50000 (80%)]\tLoss: 1.964037\n",
      "Train Epoch: 1 [40128/50000 (80%)]\tLoss: 1.868020\n",
      "Train Epoch: 1 [40192/50000 (80%)]\tLoss: 2.061724\n",
      "Train Epoch: 1 [40256/50000 (80%)]\tLoss: 1.878810\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 2.180880\n",
      "Train Epoch: 1 [40384/50000 (81%)]\tLoss: 2.222718\n",
      "Train Epoch: 1 [40448/50000 (81%)]\tLoss: 2.050713\n",
      "Train Epoch: 1 [40512/50000 (81%)]\tLoss: 2.113615\n",
      "Train Epoch: 1 [40576/50000 (81%)]\tLoss: 1.939428\n",
      "Train Epoch: 1 [40640/50000 (81%)]\tLoss: 2.021071\n",
      "Train Epoch: 1 [40704/50000 (81%)]\tLoss: 1.884396\n",
      "Train Epoch: 1 [40768/50000 (81%)]\tLoss: 1.765604\n",
      "Train Epoch: 1 [40832/50000 (82%)]\tLoss: 1.977661\n",
      "Train Epoch: 1 [40896/50000 (82%)]\tLoss: 2.085388\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.104782\n",
      "Train Epoch: 1 [41024/50000 (82%)]\tLoss: 1.933289\n",
      "Train Epoch: 1 [41088/50000 (82%)]\tLoss: 1.905280\n",
      "Train Epoch: 1 [41152/50000 (82%)]\tLoss: 2.044280\n",
      "Train Epoch: 1 [41216/50000 (82%)]\tLoss: 1.834308\n",
      "Train Epoch: 1 [41280/50000 (82%)]\tLoss: 1.932943\n",
      "Train Epoch: 1 [41344/50000 (83%)]\tLoss: 2.007008\n",
      "Train Epoch: 1 [41408/50000 (83%)]\tLoss: 1.927982\n",
      "Train Epoch: 1 [41472/50000 (83%)]\tLoss: 1.927076\n",
      "Train Epoch: 1 [41536/50000 (83%)]\tLoss: 1.978846\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.962188\n",
      "Train Epoch: 1 [41664/50000 (83%)]\tLoss: 1.898586\n",
      "Train Epoch: 1 [41728/50000 (83%)]\tLoss: 1.992734\n",
      "Train Epoch: 1 [41792/50000 (84%)]\tLoss: 1.990202\n",
      "Train Epoch: 1 [41856/50000 (84%)]\tLoss: 1.823707\n",
      "Train Epoch: 1 [41920/50000 (84%)]\tLoss: 2.114810\n",
      "Train Epoch: 1 [41984/50000 (84%)]\tLoss: 1.975250\n",
      "Train Epoch: 1 [42048/50000 (84%)]\tLoss: 2.211998\n",
      "Train Epoch: 1 [42112/50000 (84%)]\tLoss: 2.206797\n",
      "Train Epoch: 1 [42176/50000 (84%)]\tLoss: 1.973956\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 2.213923\n",
      "Train Epoch: 1 [42304/50000 (85%)]\tLoss: 2.023188\n",
      "Train Epoch: 1 [42368/50000 (85%)]\tLoss: 2.083843\n",
      "Train Epoch: 1 [42432/50000 (85%)]\tLoss: 1.897943\n",
      "Train Epoch: 1 [42496/50000 (85%)]\tLoss: 1.964726\n",
      "Train Epoch: 1 [42560/50000 (85%)]\tLoss: 2.248533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [42624/50000 (85%)]\tLoss: 1.970963\n",
      "Train Epoch: 1 [42688/50000 (85%)]\tLoss: 2.227337\n",
      "Train Epoch: 1 [42752/50000 (85%)]\tLoss: 2.084829\n",
      "Train Epoch: 1 [42816/50000 (86%)]\tLoss: 1.950737\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 1.978538\n",
      "Train Epoch: 1 [42944/50000 (86%)]\tLoss: 2.067226\n",
      "Train Epoch: 1 [43008/50000 (86%)]\tLoss: 2.078316\n",
      "Train Epoch: 1 [43072/50000 (86%)]\tLoss: 2.095318\n",
      "Train Epoch: 1 [43136/50000 (86%)]\tLoss: 1.845566\n",
      "Train Epoch: 1 [43200/50000 (86%)]\tLoss: 1.897295\n",
      "Train Epoch: 1 [43264/50000 (86%)]\tLoss: 1.848220\n",
      "Train Epoch: 1 [43328/50000 (87%)]\tLoss: 1.939601\n",
      "Train Epoch: 1 [43392/50000 (87%)]\tLoss: 2.117101\n",
      "Train Epoch: 1 [43456/50000 (87%)]\tLoss: 2.075598\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.890546\n",
      "Train Epoch: 1 [43584/50000 (87%)]\tLoss: 2.016797\n",
      "Train Epoch: 1 [43648/50000 (87%)]\tLoss: 2.023373\n",
      "Train Epoch: 1 [43712/50000 (87%)]\tLoss: 2.285724\n",
      "Train Epoch: 1 [43776/50000 (87%)]\tLoss: 1.975192\n",
      "Train Epoch: 1 [43840/50000 (88%)]\tLoss: 2.085768\n",
      "Train Epoch: 1 [43904/50000 (88%)]\tLoss: 2.082269\n",
      "Train Epoch: 1 [43968/50000 (88%)]\tLoss: 2.028042\n",
      "Train Epoch: 1 [44032/50000 (88%)]\tLoss: 2.075407\n",
      "Train Epoch: 1 [44096/50000 (88%)]\tLoss: 1.933310\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 1.907152\n",
      "Train Epoch: 1 [44224/50000 (88%)]\tLoss: 1.903505\n",
      "Train Epoch: 1 [44288/50000 (88%)]\tLoss: 1.882396\n",
      "Train Epoch: 1 [44352/50000 (89%)]\tLoss: 2.006515\n",
      "Train Epoch: 1 [44416/50000 (89%)]\tLoss: 2.268978\n",
      "Train Epoch: 1 [44480/50000 (89%)]\tLoss: 1.983096\n",
      "Train Epoch: 1 [44544/50000 (89%)]\tLoss: 2.020995\n",
      "Train Epoch: 1 [44608/50000 (89%)]\tLoss: 2.042176\n",
      "Train Epoch: 1 [44672/50000 (89%)]\tLoss: 1.917566\n",
      "Train Epoch: 1 [44736/50000 (89%)]\tLoss: 1.943449\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.165533\n",
      "Train Epoch: 1 [44864/50000 (90%)]\tLoss: 2.181881\n",
      "Train Epoch: 1 [44928/50000 (90%)]\tLoss: 2.057186\n",
      "Train Epoch: 1 [44992/50000 (90%)]\tLoss: 1.865611\n",
      "Train Epoch: 1 [45056/50000 (90%)]\tLoss: 1.899108\n",
      "Train Epoch: 1 [45120/50000 (90%)]\tLoss: 1.844111\n",
      "Train Epoch: 1 [45184/50000 (90%)]\tLoss: 2.037765\n",
      "Train Epoch: 1 [45248/50000 (90%)]\tLoss: 1.944881\n",
      "Train Epoch: 1 [45312/50000 (91%)]\tLoss: 2.103425\n",
      "Train Epoch: 1 [45376/50000 (91%)]\tLoss: 1.857365\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 2.152558\n",
      "Train Epoch: 1 [45504/50000 (91%)]\tLoss: 2.212051\n",
      "Train Epoch: 1 [45568/50000 (91%)]\tLoss: 1.965496\n",
      "Train Epoch: 1 [45632/50000 (91%)]\tLoss: 1.991819\n",
      "Train Epoch: 1 [45696/50000 (91%)]\tLoss: 2.069383\n",
      "Train Epoch: 1 [45760/50000 (91%)]\tLoss: 2.051456\n",
      "Train Epoch: 1 [45824/50000 (92%)]\tLoss: 1.992030\n",
      "Train Epoch: 1 [45888/50000 (92%)]\tLoss: 2.164040\n",
      "Train Epoch: 1 [45952/50000 (92%)]\tLoss: 2.072071\n",
      "Train Epoch: 1 [46016/50000 (92%)]\tLoss: 1.938018\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 2.176234\n",
      "Train Epoch: 1 [46144/50000 (92%)]\tLoss: 2.082443\n",
      "Train Epoch: 1 [46208/50000 (92%)]\tLoss: 1.906870\n",
      "Train Epoch: 1 [46272/50000 (92%)]\tLoss: 2.095772\n",
      "Train Epoch: 1 [46336/50000 (93%)]\tLoss: 2.079348\n",
      "Train Epoch: 1 [46400/50000 (93%)]\tLoss: 2.062145\n",
      "Train Epoch: 1 [46464/50000 (93%)]\tLoss: 1.969708\n",
      "Train Epoch: 1 [46528/50000 (93%)]\tLoss: 1.803015\n",
      "Train Epoch: 1 [46592/50000 (93%)]\tLoss: 2.025992\n",
      "Train Epoch: 1 [46656/50000 (93%)]\tLoss: 1.874823\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 2.087221\n",
      "Train Epoch: 1 [46784/50000 (93%)]\tLoss: 2.155196\n",
      "Train Epoch: 1 [46848/50000 (94%)]\tLoss: 1.869437\n",
      "Train Epoch: 1 [46912/50000 (94%)]\tLoss: 1.839668\n",
      "Train Epoch: 1 [46976/50000 (94%)]\tLoss: 2.180851\n",
      "Train Epoch: 1 [47040/50000 (94%)]\tLoss: 1.953853\n",
      "Train Epoch: 1 [47104/50000 (94%)]\tLoss: 1.882715\n",
      "Train Epoch: 1 [47168/50000 (94%)]\tLoss: 1.867012\n",
      "Train Epoch: 1 [47232/50000 (94%)]\tLoss: 2.120245\n",
      "Train Epoch: 1 [47296/50000 (95%)]\tLoss: 1.918268\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 2.083606\n",
      "Train Epoch: 1 [47424/50000 (95%)]\tLoss: 1.927738\n",
      "Train Epoch: 1 [47488/50000 (95%)]\tLoss: 1.843826\n",
      "Train Epoch: 1 [47552/50000 (95%)]\tLoss: 2.100855\n",
      "Train Epoch: 1 [47616/50000 (95%)]\tLoss: 1.925264\n",
      "Train Epoch: 1 [47680/50000 (95%)]\tLoss: 1.886033\n",
      "Train Epoch: 1 [47744/50000 (95%)]\tLoss: 1.919400\n",
      "Train Epoch: 1 [47808/50000 (96%)]\tLoss: 1.812871\n",
      "Train Epoch: 1 [47872/50000 (96%)]\tLoss: 1.949723\n",
      "Train Epoch: 1 [47936/50000 (96%)]\tLoss: 2.177321\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.943168\n",
      "Train Epoch: 1 [48064/50000 (96%)]\tLoss: 1.966395\n",
      "Train Epoch: 1 [48128/50000 (96%)]\tLoss: 1.961796\n",
      "Train Epoch: 1 [48192/50000 (96%)]\tLoss: 1.946745\n",
      "Train Epoch: 1 [48256/50000 (96%)]\tLoss: 2.081882\n",
      "Train Epoch: 1 [48320/50000 (97%)]\tLoss: 1.993915\n",
      "Train Epoch: 1 [48384/50000 (97%)]\tLoss: 2.152985\n",
      "Train Epoch: 1 [48448/50000 (97%)]\tLoss: 2.068476\n",
      "Train Epoch: 1 [48512/50000 (97%)]\tLoss: 2.145075\n",
      "Train Epoch: 1 [48576/50000 (97%)]\tLoss: 2.104676\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.844599\n",
      "Train Epoch: 1 [48704/50000 (97%)]\tLoss: 2.002245\n",
      "Train Epoch: 1 [48768/50000 (97%)]\tLoss: 2.014815\n",
      "Train Epoch: 1 [48832/50000 (98%)]\tLoss: 1.962938\n",
      "Train Epoch: 1 [48896/50000 (98%)]\tLoss: 2.045824\n",
      "Train Epoch: 1 [48960/50000 (98%)]\tLoss: 1.848758\n",
      "Train Epoch: 1 [49024/50000 (98%)]\tLoss: 2.007943\n",
      "Train Epoch: 1 [49088/50000 (98%)]\tLoss: 1.894656\n",
      "Train Epoch: 1 [49152/50000 (98%)]\tLoss: 1.928716\n",
      "Train Epoch: 1 [49216/50000 (98%)]\tLoss: 2.028728\n",
      "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 2.295412\n",
      "Train Epoch: 1 [49344/50000 (99%)]\tLoss: 2.103608\n",
      "Train Epoch: 1 [49408/50000 (99%)]\tLoss: 1.940129\n",
      "Train Epoch: 1 [49472/50000 (99%)]\tLoss: 2.006261\n",
      "Train Epoch: 1 [49536/50000 (99%)]\tLoss: 1.943802\n",
      "Train Epoch: 1 [49600/50000 (99%)]\tLoss: 1.917744\n",
      "Train Epoch: 1 [49664/50000 (99%)]\tLoss: 2.144020\n",
      "Train Epoch: 1 [49728/50000 (99%)]\tLoss: 2.188271\n",
      "Train Epoch: 1 [49792/50000 (99%)]\tLoss: 1.738500\n",
      "Train Epoch: 1 [49856/50000 (100%)]\tLoss: 1.944140\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 2.160038\n",
      "Train Epoch: 1 [12496/50000 (100%)]\tLoss: 2.086172\n",
      "Files already downloaded and verified\n",
      "1\n",
      "tensor(1) tensor([0.4492])\n",
      "tensor(9) tensor([0.7517])\n",
      "tensor(9) tensor([0.5387])\n",
      "tensor(1) tensor([0.4386])\n",
      "tensor(9) tensor([0.5063])\n",
      "tensor(9) tensor([0.4932])\n",
      "tensor(9) tensor([0.5410])\n",
      "tensor(9) tensor([0.3987])\n",
      "tensor(1) tensor([0.4881])\n",
      "label 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [10, 3, 5, 5], but got input of size [1, 1, 2, 32, 32] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4a601f50eb6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/foolbox/models/base.py\u001b[0m in \u001b[0;36mpredictions\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/foolbox/models/pytorch.py\u001b[0m in \u001b[0;36mbatch_predictions\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;31m# TODO: add no_grad once we have a solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# for models that require grads internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5d3851552c3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         rebuild_parameters(self.var_model.dico, self.var_model.model,\n\u001b[1;32m    380\u001b[0m                            _epsilon_setting)\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a601f50eb6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [10, 3, 5, 5], but got input of size [1, 1, 2, 32, 32] instead"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "\n",
    "# Simple CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.convm1 = nn.Conv2d(10,10, kernel_size=3, padding=1)\n",
    "        self.convm2 = nn.Conv2d(10,10, kernel_size=3, padding=1)\n",
    "        self.convm3 = nn.Conv2d(10,10, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(500, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.bn2 = nn.BatchNorm2d(20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(F.max_pool2d(self.conv1(x), 2)))\n",
    "        x = self.bn1(F.relu(self.convm1(x)))\n",
    "        x = self.bn1(F.relu(self.convm2(x)))\n",
    "        x = self.bn1(F.relu(self.convm3(x)))\n",
    "        x = self.bn2(F.relu(F.max_pool2d(self.conv2(x), 2)))\n",
    "        x = x.view(-1, 500)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "        \n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Data loading code\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR10('./files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "\n",
    "net = VGG('VGG11')\n",
    "x = torch.randn(2,3,32,32)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "var_model = Variationalize(model)\n",
    "\n",
    "\n",
    "                      \n",
    "optimizer = optim.Adam(var_model.parameters(), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    var_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data, target\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = var_model(data)\n",
    "        loss_error = F.nll_loss(output, target)\n",
    "        \n",
    "        loss_prior = var_model.prior_loss() / 60000\n",
    "        loss = loss_error + loss_prior\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch)\n",
    "      \n",
    "      \n",
    "#examples = enumerate(test_loader)\n",
    "#batch_idx, (data, target) = next(examples)\n",
    "\n",
    "cifar10_trainset = torchvision.datasets.CIFAR10(root='./files', train=False, download=True, transform=None)\n",
    "test_image_zero, test_target_zero = cifar10_trainset[6]\n",
    "\n",
    "target = test_target_zero\n",
    "\n",
    "loader = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "data = loader(test_image_zero).float()\n",
    "\n",
    "plt.figure(0)\n",
    "plt.imshow(test_image_zero)\n",
    "\n",
    "data = Variable(data, requires_grad=True)\n",
    "data = data.unsqueeze(0)  \n",
    "\n",
    "# Test prediction\n",
    "var_model.eval()\n",
    "# Use PyVarInf's Sample wrapper to sample models from the variational distribution using draw()\n",
    "var_model = Sample(var_model)\n",
    "print(target)\n",
    "for i in range (1, 10):\n",
    "    data, target = data, target\n",
    "    var_model.draw()\n",
    "    result = var_model(data).detach()\n",
    "    values, index = torch.max(result,1)\n",
    "    conf = np.exp(result[0][index])\n",
    "    print(index[0], conf)\n",
    "        \n",
    "\n",
    "# Convert data and target to Foolbox friendly format\n",
    "data = data[:, :2]\n",
    "data = data.data.cpu().numpy()\n",
    "#target = target.data.cpu().numpy()\n",
    "\n",
    "var_model.eval()\n",
    "# Convert model to Foolbox model\n",
    "fmodel = foolbox.models.PyTorchModel(\n",
    "    var_model, bounds=(0, 1), num_classes=10)\n",
    "\n",
    "print('label', target)\n",
    "result = np.exp(fmodel.predictions(data))\n",
    "pred = np.argmax(result)\n",
    "\n",
    "# Black Box Attack Model\n",
    "print('predicted class', pred, '| confidence', result[pred]);\n",
    "# apply attack on source image\n",
    "attack = foolbox.attacks.FGSM(fmodel)\n",
    "adversarial = attack(data, target)\n",
    "\n",
    "im = Image.fromarray(np.uint8(adversarial[0]*255))\n",
    "adversarial = loader(im).float()\n",
    "adversarial = Variable(adversarial, requires_grad=True)\n",
    "adversarial = adversarial.unsqueeze(0)  \n",
    "for i in range (1, 10):\n",
    "    attack_data = adversarial\n",
    "    var_model.draw()\n",
    "    result = var_model(attack_data).detach()\n",
    "    values, index = torch.max(result,1)\n",
    "    conf = np.exp(result[0][index])\n",
    "    print(index[0], conf)\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
